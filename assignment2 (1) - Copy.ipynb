{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e902f-783e-409b-a748-d217a4cd3036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Write python code to classify the heart disease data using a support vector machine and evaluate the model using various performance evaluation metrics as discussed. \n",
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "# Separating the features (X) and target (y)\n",
    "X = data.drop('target', axis=1)  # Replace 'target' with  actual target column name\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Support Vector Machine model \n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "y_prob = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "\n",
    "# Plotting the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the confusion matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd6c30-1ebc-4e3e-8f36-3af0edc5c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the heart disease data statistical analysis and plot the various features individually.\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset \n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First five rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Get basic statistical information\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Get information about categorical features\n",
    "print(\"\\nData Types:\")\n",
    "print(data.info())\n",
    "\n",
    "# Plot histograms for continuous features\n",
    "data.hist(bins=15, figsize=(15, 10), color='steelblue', edgecolor='black')\n",
    "plt.suptitle('Histograms of Continuous Features')\n",
    "plt.show()\n",
    "\n",
    "# Plot boxplots for continuous features to check for outliers\n",
    "plt.figure(figsize=(15, 10))\n",
    "data.plot(kind='box', subplots=True, layout=(4, 4), sharex=False, sharey=False, figsize=(15, 10), patch_artist=True)\n",
    "plt.suptitle('Box Plots of Continuous Features')\n",
    "plt.show()\n",
    "\n",
    "# Plot correlation matrix to explore relationships between features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot count plots for categorical features\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'] \n",
    "\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data[feature], palette='Set2')\n",
    "    plt.title(f'Count Plot for {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89d332-3a4c-428c-bd4d-162c6c06533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use these methods to normalize the following group of data: 200,300,400,600,1000  \n",
    "#a. min-max normalization by setting min = 0 and max = 1  \n",
    "#b. z-score normalization  \n",
    "#c. z-score normalization using the mean absolute deviation instead of standard \n",
    "##d. normalization by decimal scaling\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "data = np.array([200, 300, 400, 600, 1000])\n",
    "\n",
    "# a. Min-Max Normalization (min = 0, max = 1)\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "min_max_normalized = (data - min_val) / (max_val - min_val)\n",
    "\n",
    "# b. Z-Score Normalization\n",
    "mean = np.mean(data)\n",
    "std_dev = np.std(data)\n",
    "z_score_normalized = (data - mean) / std_dev\n",
    "\n",
    "# c. Z-Score Normalization using Mean Absolute Deviation \n",
    "mad = np.mean(np.abs(data - mean))\n",
    "z_score_mad_normalized = (data - mean) / mad\n",
    "\n",
    "# d. Normalization by Decimal Scaling\n",
    "j = np.ceil(np.log10(np.max(np.abs(data))))\n",
    "decimal_scaling_normalized = data / (10**j)\n",
    "\n",
    "# Display results\n",
    "print(\"Original Data: \", data)\n",
    "print(\"\\nMin-Max Normalized Data: \", min_max_normalized)\n",
    "print(\"\\nZ-Score Normalized Data: \", z_score_normalized)\n",
    "print(\"\\nZ-Score (MAD) Normalized Data: \", z_score_mad_normalized)\n",
    "print(\"\\nDecimal Scaling Normalized Data: \", decimal_scaling_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13a506-84ab-43bb-bb81-bcc08dba579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The \"Plant Growth Data Classification\" dataset, the prediction task would typically involve predicting or classifying the growth milestone of plants based on the provided \n",
    "#environmental and management factors. Specifically, you would aim to predict the growth stage or milestone that a plant reaches based on variables such as soil type, \n",
    "#sunlight hours, water frequency, fertilizer type, temperature, and humidity. This \n",
    "#prediction can help in understanding how different conditions influence plant growth and can be valuable for optimizing agricultural practices or greenhouse management. \n",
    "#Perform classification of the dataset using deep neural network. \n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the dataset \n",
    "\n",
    "data = pd.read_csv('plant_growth.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop('growth_stage', axis=1)  # Drop the target column \n",
    "y = data['growth_stage']  # Target column with categorical values \n",
    "\n",
    "# Convert categorical target\n",
    "y, class_names = pd.factorize(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# One-hot encode the target labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Build the Deep Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer (number of classes should match the number of unique growth stages)\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Predictions and classification report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report and accuracy score\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "print(f\"\\nAccuracy Score: {accuracy_score(y_true_classes, y_pred_classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31858452-a9ba-48b4-8177-3a8d4cb8a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the student grades using Linear Regression, Random Forest, Support Vector Machine.\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset \n",
    "# Assuming the dataset has columns like 'study_hours', 'attendance', 'previous_grades', etc.\n",
    "data = pd.read_csv('student_grades.csv')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop('final_grade', axis=1)  \n",
    "y = data['final_grade'] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 1. Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# 2. Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 3. Support Vector Regressor (SVM)\n",
    "svr_model = SVR(kernel='linear') \n",
    "svr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svr = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"RÂ² Score: {r2:.2f}\")\n",
    "\n",
    "# Evaluate Linear Regression\n",
    "evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# Evaluate Support Vector Regressor\n",
    "evaluate_model(y_test, y_pred_svr, \"Support Vector Machine\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
